{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv7 Braking Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CT0Q8M_CY_Tn"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArmaanSinghSandhu/YOLOv7-Braking-Detection/blob/main/YOLOv7_Braking_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training YOLOv7 to detect the vehicle braking light status for preemptive braking**"
      ],
      "metadata": {
        "id": "5vA34rUMDsIZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project is based on the [YOLOv7 official repository](https://github.com/WongKinYiu/yolov7). YOLOv7 is the latest YOLO version and according to the [paper](https://arxiv.org/abs/2207.02696), the fastest and the most accurate version till date.\n",
        "\n",
        "This project trains YOLOv7 on a custom dataset to detect the leading vehicle's braking from its tail light status.\n",
        "\n",
        "**Steps for training**\n",
        "\n",
        "\n",
        "*   Mount Google Drive\n",
        "*   Download YOLOv7 from the official repository\n",
        "*   Install YOLOv7 dependecies\n",
        "*   Download pretrained model\n",
        "*   Prepare the custom dataset\n",
        "*   Run training\n",
        "*   Evaluate training performance\n",
        "*   Run inference on images and videos\n",
        "\n"
      ],
      "metadata": {
        "id": "_sX-RP8CEXKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "81l9w6OaX4OI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYtdlhmVXlrF",
        "outputId": "52f3e503-c60c-49cc-ae83-803ce4484cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download YOLOv7 repository\n"
      ],
      "metadata": {
        "id": "sqjx2gJtV4a3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAwLVSQ5hH8T",
        "outputId": "eb09352c-6cdf-428a-a13d-d12876435195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov7'...\n",
            "remote: Enumerating objects: 724, done.\u001b[K\n",
            "remote: Counting objects: 100% (724/724), done.\u001b[K\n",
            "remote: Compressing objects: 100% (379/379), done.\u001b[K\n",
            "remote: Total 724 (delta 366), reused 644 (delta 330), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (724/724), 66.91 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (366/366), done.\n",
            "Checking out files: 100% (102/102), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WongKinYiu/yolov7.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install YOLOv7 dependencies"
      ],
      "metadata": {
        "id": "VHzh9riyYUH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov7\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0RchVOIjaFj",
        "outputId": "a26c5680-c2d2-4628-e100-f57d917a581c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.7.3)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision!=0.13.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.64.0)\n",
            "Requirement already satisfied: protobuf<4.21.3 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 14)) (3.17.3)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 17)) (2.8.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 22)) (0.11.2)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 34)) (5.5.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 35)) (5.4.8)\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 36)) (0.1.1.post2207130030)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.12.0,>=1.7.0->-r requirements.txt (line 11)) (4.1.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4.21.3->-r requirements.txt (line 14)) (1.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.47.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.6.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 21)) (2022.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 17)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 17)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 17)) (3.2.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r requirements.txt (line 34)) (4.4.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r requirements.txt (line 34)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->-r requirements.txt (line 34)) (0.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download pretrained model"
      ],
      "metadata": {
        "id": "XoJ7E2YqYpuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HMEvLyjkklj",
        "outputId": "94bf0f4c-cb50-43c7-ec5b-dd7a56defe98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-12 02:43:37--  https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220812%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220812T024337Z&X-Amz-Expires=300&X-Amz-Signature=4ed5d9e3cf58ced66a47af3f7ff5eeb3b3c89eea693cdcb216eac25fde0d7735&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-08-12 02:43:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/511187726/b0243edf-9fb0-4337-95e1-42555f1b37cf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220812%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220812T024337Z&X-Amz-Expires=300&X-Amz-Signature=4ed5d9e3cf58ced66a47af3f7ff5eeb3b3c89eea693cdcb216eac25fde0d7735&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=511187726&response-content-disposition=attachment%3B%20filename%3Dyolov7.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75587165 (72M) [application/octet-stream]\n",
            "Saving to: ‘yolov7.pt’\n",
            "\n",
            "yolov7.pt           100%[===================>]  72.08M  4.83MB/s    in 9.1s    \n",
            "\n",
            "2022-08-12 02:43:47 (7.90 MB/s) - ‘yolov7.pt’ saved [75587165/75587165]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the dataset\n",
        "\n",
        "The dataset for this project comprises of the rear signal dataset available [here](http://vllab1.ucmerced.edu/~hhsu22/rear_signal/rear_signal). This dataset is credited to:\n",
        "\n",
        "Hsu, H. K., Tsai, Y. H., Mei, X., Lee, K. H., Nagasaka, N., Prokhorov, D., & Yang, M. H. (2017, October). Learning to tell brake and turn signals in videos using cnn-lstm structure. In 2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC) (pp. 1-6). IEEE.\n",
        "\n",
        "This dataset contains cropped images of vehicle tail lights in various stages and combinations of braking and turning from a [research](https://drive.google.com/open?id=13bCTSnB-29U83QgmLWihMXlqErMmJy-E) using CNN-LTSM model to classify vehicle tail lights. While the dataset is optimal for use with a classification model, it would require background augmentation for a detection algorithm like YOLO. For the background images, the GTSDB dataset was used.\n",
        "\n",
        "The vehicle images were filtered on the basis of vehicle diversity, sizes, angles, lighting, resolution and duplicates. It was ensured that there were equal number of images for each detection class. While the backgrounds were filtered on the basis of lighting, diversity, shots of long & open roads/highways, minimal to no leading traffic.\n",
        "\n",
        "The vehicle images were then sorted into two main catagories:\n",
        "\n",
        "1. Braking: All the images where the braking lights are on (including Braking, Braking & Turning Left, Braking & Turning Right).\n",
        "\n",
        "2. Normal (Non Braking): All the images where the braking lights are off (including Off, Turning Left, Turning Right).\n",
        "\n",
        "\n",
        "After the images were sorted, the sorted images were augmented by adding them to the backgrounds during which they were annotated using the custom code and preprocessing steps detailed in the [Dataset_Prep.ipynb](https://colab.research.google.com/drive/1HMgSZYyB3FT-9LmTVFBHszsHEg7h-Dk8?usp=sharing) . Although there are various tools available for this purpose, the custom code helped automate the augmentation, labelling and data splitting process. For a smaller volume dataset, manual preprocessing is recommended.\n",
        " "
      ],
      "metadata": {
        "id": "CT0Q8M_CY_Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Training\n",
        "\n",
        "The training process uses YOLOv7 pretrained weights [downloaded](https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt) from the official YOLO repository. Details about all the train.py command line arguments can be found [here](https://github.com/WongKinYiu/yolov7/blob/6ded32cc8d0ab40ea51f385876c143011ec69197/train.py#L526). Few of the ones used here are:\n",
        "\n",
        "**--weights**: path to the pretrained weights downloaded earlier\n",
        "\n",
        "**--data**: path to the yaml file containing details about dataset path and object classes.\n",
        "\n",
        "**--batch-size**: The number of subsets/parts the dataset is divided into to pass through the network per epoch. Depends on the size of the dataset and the GPU memory.\n",
        "\n",
        "**--device**: Processing hardware, 0 for single GPU, 0,1,.. for multiple parallel GPUs, CPU for CPU. \n",
        "\n",
        "**--img-size**: Image size for training.\n",
        "\n",
        "**--cfg**: Path to the yaml config file appropriate to the training model\n",
        "\n",
        "**--epochs**: Number of training epochs"
      ],
      "metadata": {
        "id": "pOoS-fZEj9-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --weights yolov7.pt --data data/custom.yaml --batch-size 8 --device 0 --img-size 800 --cfg cfg/training/yolov7.yaml --epochs 40"
      ],
      "metadata": {
        "id": "ZiAHCXYR8Hw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b8bd27-52cc-45fb-82c5-90996058b75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLOR 🚀 v0.1-101-g1b63720 torch 1.12.1+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Namespace(adam=False, artifact_alias='latest', batch_size=8, bbox_interval=-1, bucket='', cache_images=True, cfg='cfg/training/yolov7.yaml', data='data/custom.yaml', device='0', entity=None, epochs=1, evolve=False, exist_ok=False, freeze=[0], global_rank=-1, hyp='data/hyp.scratch.p5.yaml', image_weights=False, img_size=[800, 800], label_smoothing=0.0, linear_lr=False, local_rank=-1, multi_scale=False, name='exp', noautoanchor=False, nosave=False, notest=False, project='runs/train', quad=False, rect=False, resume=False, save_dir='runs/train/exp5', save_period=-1, single_cls=False, sync_bn=False, total_batch_size=8, upload_dataset=False, weights='yolov7.pt', workers=8, world_size=1)\n",
            "\u001b[34m\u001b[1mtensorboard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.3, cls_pw=1.0, obj=0.7, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.2, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.0, paste_in=0.15, loss_ota=1\n",
            "\u001b[34m\u001b[1mwandb: \u001b[0mInstall Weights & Biases for YOLOR logging with 'pip install wandb' (recommended)\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  5                -2  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            "  6                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  7                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  8                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            "  9                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 10  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 11                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 12                -1  1         0  models.common.MP                        []                            \n",
            " 13                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 14                -3  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 16          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 18                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 19                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 20                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 22                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 23  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 24                -1  1    263168  models.common.Conv                      [512, 512, 1, 1]              \n",
            " 25                -1  1         0  models.common.MP                        []                            \n",
            " 26                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 27                -3  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 28                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 29          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 30                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 31                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 32                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 33                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 34                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 35                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 36  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 37                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 38                -1  1         0  models.common.MP                        []                            \n",
            " 39                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 40                -3  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 41                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 42          [-1, -3]  1         0  models.common.Concat                    [1]                           \n",
            " 43                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 44                -2  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 45                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 46                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 47                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 48                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 49  [-1, -3, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 50                -1  1   1050624  models.common.Conv                      [1024, 1024, 1, 1]            \n",
            " 51                -1  1   7609344  models.common.SPPCSPC                   [1024, 512, 1]                \n",
            " 52                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 53                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 54                37  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 55          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 56                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 57                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 58                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 59                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 60                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 61                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 62[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 63                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 64                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 65                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 66                24  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 67          [-1, -2]  1         0  models.common.Concat                    [1]                           \n",
            " 68                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 69                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 70                -1  1     73856  models.common.Conv                      [128, 64, 3, 1]               \n",
            " 71                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 72                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 73                -1  1     36992  models.common.Conv                      [64, 64, 3, 1]                \n",
            " 74[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 75                -1  1     65792  models.common.Conv                      [512, 128, 1, 1]              \n",
            " 76                -1  1         0  models.common.MP                        []                            \n",
            " 77                -1  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 78                -3  1     16640  models.common.Conv                      [128, 128, 1, 1]              \n",
            " 79                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 80      [-1, -3, 63]  1         0  models.common.Concat                    [1]                           \n",
            " 81                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 82                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 83                -1  1    295168  models.common.Conv                      [256, 128, 3, 1]              \n",
            " 84                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 85                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 86                -1  1    147712  models.common.Conv                      [128, 128, 3, 1]              \n",
            " 87[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            " 88                -1  1    262656  models.common.Conv                      [1024, 256, 1, 1]             \n",
            " 89                -1  1         0  models.common.MP                        []                            \n",
            " 90                -1  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 91                -3  1     66048  models.common.Conv                      [256, 256, 1, 1]              \n",
            " 92                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 93      [-1, -3, 51]  1         0  models.common.Concat                    [1]                           \n",
            " 94                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 95                -2  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 96                -1  1   1180160  models.common.Conv                      [512, 256, 3, 1]              \n",
            " 97                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 98                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            " 99                -1  1    590336  models.common.Conv                      [256, 256, 3, 1]              \n",
            "100[-1, -2, -3, -4, -5, -6]  1         0  models.common.Concat                    [1]                           \n",
            "101                -1  1   1049600  models.common.Conv                      [2048, 512, 1, 1]             \n",
            "102                75  1    328704  models.common.RepConv                   [128, 256, 3, 1]              \n",
            "103                88  1   1312768  models.common.RepConv                   [256, 512, 3, 1]              \n",
            "104               101  1   5246976  models.common.RepConv                   [512, 1024, 3, 1]             \n",
            "105   [102, 103, 104]  1     39550  models.yolo.IDetect                     [2, [[12, 16, 19, 36, 40, 28], [36, 75, 76, 55, 72, 146], [142, 110, 192, 243, 459, 401]], [256, 512, 1024]]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Model Summary: 415 layers, 37201950 parameters, 37201950 gradients, 105.1 GFLOPS\n",
            "\n",
            "Transferred 552/566 items from yolov7.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "Optimizer groups: 95 .bias, 95 conv.weight, 98 other\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 88 found, 0 missing, 0 empty, 0 corrupted:   3% 88/3200 [02:01<33:26,  1.55it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-110.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 234 found, 0 missing, 0 empty, 1 corrupted:   7% 234/3200 [03:37<33:17,  1.48it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1261.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 357 found, 0 missing, 0 empty, 2 corrupted:  11% 357/3200 [05:00<31:15,  1.52it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1395.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 712 found, 0 missing, 0 empty, 3 corrupted:  22% 712/3200 [08:59<27:31,  1.51it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1788.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 737 found, 0 missing, 0 empty, 4 corrupted:  23% 737/3200 [09:16<27:34,  1.49it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1812.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 768 found, 0 missing, 0 empty, 5 corrupted:  24% 768/3200 [09:36<26:05,  1.55it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1847.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 790 found, 0 missing, 0 empty, 6 corrupted:  25% 790/3200 [09:51<28:15,  1.42it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-1873.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 1002 found, 0 missing, 0 empty, 7 corrupted:  31% 1002/3200 [12:14<24:19,  1.51it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-311.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 1201 found, 0 missing, 0 empty, 8 corrupted:  38% 1201/3200 [14:27<22:33,  1.48it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-532.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 1384 found, 0 missing, 0 empty, 9 corrupted:  43% 1384/3200 [16:28<20:08,  1.50it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-743.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 1528 found, 0 missing, 0 empty, 10 corrupted:  48% 1528/3200 [18:06<18:36,  1.50it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-909.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 1533 found, 0 missing, 0 empty, 11 corrupted:  48% 1533/3200 [18:09<19:42,  1.41it/s]\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/train/images/Braking-915.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning 'dataset/train/labels' images and labels... 3200 found, 0 missing, 0 empty, 12 corrupted: 100% 3200/3200 [36:53<00:00,  1.45it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.1GB): 100% 3188/3188 [00:14<00:00, 222.64it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'dataset/valid/labels' images and labels... 72 found, 0 missing, 0 empty, 0 corrupted:  18% 72/400 [00:53<03:32,  1.54it/s]\u001b[34m\u001b[1mval: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/valid/images/Braking-1720.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'dataset/valid/labels' images and labels... 110 found, 0 missing, 0 empty, 1 corrupted:  28% 110/400 [01:18<03:24,  1.42it/s]\u001b[34m\u001b[1mval: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/valid/images/Braking-206.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'dataset/valid/labels' images and labels... 170 found, 0 missing, 0 empty, 2 corrupted:  42% 170/400 [02:00<02:47,  1.37it/s]\u001b[34m\u001b[1mval: \u001b[0mWARNING: Ignoring corrupted image and/or label dataset/valid/images/Braking-766.jpg: non-normalized or out of bounds coordinate labels\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning 'dataset/valid/labels' images and labels... 400 found, 0 missing, 0 empty, 3 corrupted: 100% 400/400 [04:39<00:00,  1.43it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: dataset/valid/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.8GB): 100% 397/397 [00:02<00:00, 153.97it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.37, Best Possible Recall (BPR) = 1.0000\n",
            "Image sizes 800 train, 800 test\n",
            "Using 4 dataloader workers\n",
            "Logging results to runs/train/exp5\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls     total    labels  img_size\n",
            "       0/0     5.43G   0.05589    0.2446   0.01294    0.3134        10       800: 100% 399/399 [05:04<00:00,  1.31it/s]\n",
            "               Class      Images      Labels           P           R      mAP@.5  mAP@.5:.95: 100% 25/25 [00:18<00:00,  1.38it/s]\n",
            "                 all         397         397       0.514       0.884       0.511       0.331\n",
            "              Normal         397         397       0.514       0.884       0.511       0.331\n",
            "1 epochs completed in 0.092 hours.\n",
            "\n",
            "Optimizer stripped from runs/train/exp5/weights/last.pt, 74.9MB\n",
            "Optimizer stripped from runs/train/exp5/weights/best.pt, 74.9MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Results\n",
        "\n",
        "Best results were observed after training for 40 epochs as further training led to over overfitting and increase in objectness losses. <br>\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/ArmaanSinghSandhu/YOLOv7-Braking-Detection/main/results/graphs.jpeg' width=550> <br>\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/ArmaanSinghSandhu/YOLOv7-Braking-Detection/main/results/accuracy.jpeg' width=550>\n"
      ],
      "metadata": {
        "id": "E2rTf1ummlYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "Inference is performed on images and videos using the weights with best results of all the training epochs and a 50% confidence threshold. Since we are focusing on the detection of braking instances only, the 'Normal' (or non braking) class has been excluded from the video detection to increase inference speed and reduce clutter while both the classes were detected on image inputs. Details about all the detect.py command line arguments can be found [here](https://github.com/WongKinYiu/yolov7/blob/main/detect.py#L166)."
      ],
      "metadata": {
        "id": "y-gujb04oVYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights best.pt --img-size 800 --conf-thres 0.5 --source TestData --device 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzbGZI97Bkg_",
        "outputId": "e4be8599-bf64-40d7-fb61-1b1577856379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.5, device='0', exist_ok=False, img_size=800, iou_thres=0.45, name='exp', no_trace=False, nosave=False, project='runs/detect', save_conf=False, save_txt=False, source='TestData', update=False, view_img=False, weights=['best.pt'])\n",
            "YOLOR 🚀 v0.1-84-gb8956dd torch 1.12.1+cu113 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\n",
            "\n",
            "Fusing layers... \n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "RepConv.fuse_repvgg_block\n",
            "IDetect.fuse\n",
            "Model Summary: 314 layers, 36508742 parameters, 6194944 gradients\n",
            " Convert model to Traced-model... \n",
            " traced_script_module saved! \n",
            " model is traced! \n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            " The image with the result is saved in: runs/detect/exp2/rain.webp\n",
            " The output with the result is saved in: runs/detect/exp2/rain.webp\n",
            "Done. (1.748s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Results\n",
        "\n",
        "Videos and images sourced from the internet were used to run inference on using the weights with best results of all the training epochs. As previously mentioned, the inference on the videos was run while excluding the 'Normal' class since the focus was mainly on the detection of the braking instances only. While the images were detected for both the class instances. The videos were split according to the normal weather and poor weather recording conditions.\n",
        "\n",
        "The results are as follows:\n",
        "\n",
        "<font color = 'red'>(Please make sure the notebook is open in Google Colab to view the following videos and images. If viewing from Github, use the \"Open in Colab\" link here <a href=\"https://colab.research.google.com/github/ArmaanSinghSandhu/YOLOv7-Braking-Detection/blob/main/YOLOv7_Braking_Detection.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> or at the top of the notebook to open this file in Google Colab)</font>"
      ],
      "metadata": {
        "id": "5w7TE_kAa1xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<h3>Braking detection in normal weather conditions:</h3> <br> <br> <iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/zWGofdE0gsw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> <br> <br> <h3>Braking detection in poor weather conditions:</h3>  <br> <br> <iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/iL30PDUZy3I\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EZJ79_JqmKDp",
        "outputId": "6faad47b-ddf4-4ba1-d131-f30f0adf5cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Braking detection in normal weather conditions:</h3> <br> <br> <iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/zWGofdE0gsw\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> <br> <br> <h3>Braking detection in poor weather conditions:</h3>  <br> <br> <iframe width=\"800\" height=\"450\" src=\"https://www.youtube.com/embed/iL30PDUZy3I\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<h3>And the image inputs produced the following results :</h3><br><br><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQdqieG-p53H4mftLotJo25cxNTpcQTlx2IQPoXKbcY8VefaeNb8RUNBi1UcxfnXPveIVmBBiAFkSpB/embed?start=false&loop=true&delayms=30000\" frameborder=\"0\" width=\"864\" height=\"504\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe><br><br><p>The images are also available <a href = \"https://github.com/ArmaanSinghSandhu/YOLOv7-Braking-Detection/tree/main/results\" target=\"_blank\">here</a></p>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "oq6rjZpdzMks",
        "outputId": "2d2e5a68-2331-46fe-ff7f-aba97bd952c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>And the image inputs produced the following results :</h3><br><br><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQdqieG-p53H4mftLotJo25cxNTpcQTlx2IQPoXKbcY8VefaeNb8RUNBi1UcxfnXPveIVmBBiAFkSpB/embed?start=false&loop=true&delayms=30000\" frameborder=\"0\" width=\"864\" height=\"504\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe><br><br><p>The images are also available <a href = \"https://github.com/ArmaanSinghSandhu/YOLOv7-Braking-Detection/tree/main/results\" target=\"_blank\">here</a></p>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Future Enhancements\n",
        "\n",
        "The dataset and training needs to be enhanced to incorporate other vehicle classes such as bikes and heavy vehicles and various scenarios such as night driving, low lighting conditions and glares and disruptions. Also the detection of turn signals needs to be incorporated. The model performance on realtime data will also be tested. A lighter model can be trained and optimized for testing and deployment on edge devices. A future project might entail combining YOLO for detection and a classifier trained on this dataset for classification. The combination can be also used to generate an enhanced and annotated YOLO dataset."
      ],
      "metadata": {
        "id": "2yVvXzJCTnES"
      }
    }
  ]
}